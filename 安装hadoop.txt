1.下载hadoop-2.6.0
下载地址http://hadoop.apache.org/

2.解压
# tar -zxvf /..目录/hadoop-2.6.0.tar.gz -C /usr/local/hadoop

3.在hadoop目录下创建子目录
# cd /usr/local/hadoop/hadoop-2.6.0
# mkdir tmp
# mkdir name
# mkdir data

4.配置hadoop-env.sh
# cd /usr/local/hadoop/hadoop-2.6.0/etc/hadoop
# vim hadoop-env.sh
设置JAVA_HOME和PATH路径
export JAVA_HOME=/usr/local/java/jdk1.7.0_80
export PATH=$PATH:/usr/local/hadoop/hadoop-2.6.0/bin
# source hadoop-env.sh
# hadoop version
如果显示以下内容，就表示配置生效了
Hadoop 2.6.0
Subversion https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1
Compiled by jenkins on 2014-11-13T21:10Z
Compiled with protoc 2.5.0
From source with checksum 18e43357c8f927c0695f1e9522859d6a
This command was run using /usr/lib/hadoop/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar

说明：
# export HADOOP_ROOT_LOGGER=DEBUG,console //开启调试信息
# export HADOOP_ROOT_LOGGER=INFO,console  //关闭调试信息

5.配置yarn-env.sh
# cd /usr/local/hadoop/hadoop-2.6.0/etc/hadoop
# vim yarn-env.sh
设置JAVA_HOME路径
1).先找到以下内容
# some Java parameters
# export JAVA_HOME=/home/y/libexec/jdk1.6.0/
if [ "$JAVA_HOME" != "" ]; then
  #echo "run java in $JAVA_HOME"
  JAVA_HOME=$JAVA_HOME
fi
2).修改第二行配置
export JAVA_HOME=/usr/local/java/jdk1.7.0_80
# source yarn-env.sh

6.配置core-site.xml
# cd /usr/local/hadoop/hadoop-2.6.0/etc/hadoop
# vim core-site.xml
按照如下内容进行配置，fs.default.name的值是hdfs://master:9000
<configuration>
  <property>
    <name>fs.default.name</name>
    <value>hdfs://hadoop1:9000</value>
  </property>

  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://hadoop1:9000</value>
  </property>

  <property>
    <name>io.file.buffer.size</name>
    <value>131072</value>
  </property>

  <property>
    <name>hadoop.tmp.dir</name>
    <value>file:/usr/local/hadoop/hadoop-2.6.0/tmp</value>
    <description>A base for other temporary directories.</description>
  </property>

  <property>
    <name>hadoop.proxyuser.hduser.hosts</name>
    <value>*</value>
  </property>

  <property>
    <name>hadoop.proxyuser.hduser.groups</name>
    <value>*</value>
  </property>
</configuration>

7.配置hdfs-site.xml
# cd /usr/local/hadoop/hadoop-2.6.0/etc/hadoop
# vim hdfs-site.xml
按照如下内容进行配置
<configuration>
  <property>
   <name>dfs.namenode.secondary.http-address</name>
   <value>hadoop1:9001</value>
  </property>

  <property>
   <name>dfs.namenode.name.dir</name>
   <value>file:/usr/local/hadoop/hadoop-2.6.0/name</value>
  </property>

  <property>
   <name>dfs.datanode.data.dir</name>
   <value>file:/usr/local/hadoop/hadoop-2.6.0/data</value>
  </property>

  <property>
   <name>dfs.replication</name>
   <value>2</value>
  </property>

  <property>
   <name>dfs.webhdfs.enabled</name>
   <value>true</value>
  </property>
</configuration>

8.配置mapred-site.xml
# cd /usr/local/hadoop/hadoop-2.6.0/etc/hadoop
# cp mapred-site.xml.template mapred-site.xml  //默认情况下不存在mapred-site.xml文件，可以从模板拷贝一份
# vim mapred-site.xml
按照如下内容进行配置
<configuration>
  <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
  </property>

  <property>
    <name>mapreduce.jobhistory.address</name>
    <value>hadoop1:10020</value>
  </property>

  <property>
    <name>mapreduce.jobhistory.webapp.address</name>
    <value>hadoop1:19888</value>
  </property>
</configuration>

9.配置yarn-site.xml
# cd /usr/local/hadoop/hadoop-2.6.0/etc/hadoop
# vim yarn-site.xml
按照如下内容进行配置
<configuration>
  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
  </property>

  <property>
    <name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
    <value>org.apache.hadoop.mapred.ShuffleHandler</value>
  </property>

  <property>
    <name>yarn.resourcemanager.address</name>
    <value>hadoop1:8032</value>
  </property>

  <property>
    <name>yarn.resourcemanager.scheduler.address</name>
    <value>hadoop1:8030</value>
  </property>

  <property>
    <name>yarn.resourcemanager.resource-tracker.address</name>
    <value>hadoop1:8031</value>
  </property>

  <property>
    <name>yarn.resourcemanager.admin.address</name>
    <value>hadoop1:8033</value>
  </property>

  <property>
    <name>yarn.resourcemanager.webapp.address</name>
    <value>hadoop1:8088</value>
  </property>
</configuration>

10.配置Slaves文件
# cd /usr/local/hadoop/hadoop-2.6.0/etc/hadoop
# vim slaves
在文件中添加以下配置
hadoop1
hadoop2

11.向各节点分发Hadoop程序
# cd /usr/local/hadoop
# scp -r hadoop-2.6.0 root@hadoop2:/usr/local/hadoop/hadoop-2.6.0

12.格式化NameNode
# cd /usr/local/hadoop/hadoop-2.6.0
# ./bin/hdfs namenode -format
16/10/13 11:12:51 INFO common.Storage: Storage directory /usr/lib/hadoop/hadoop-2.6.0/name has been successfully formatted.
16/10/13 11:12:51 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
16/10/13 11:12:51 INFO util.ExitUtil: Exiting with status 0
16/10/13 11:12:51 INFO namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at localhost/127.0.0.1
************************************************************/

13.启动HDFS
# cd /usr/local/hadoop/hadoop-2.6.0/sbin
# ./start-dfs.sh
# jps //验证HDFS启动，hadoop1上面运行的进程有：NameNode、SecondaryNameNode和DataNode
13315 Jps
11878 DataNode
11753 NameNode
12079 SecondaryNameNode

注意：
1).NameNode启动失败，解决方法：
HDFS把namenode的格式化信息存在了系统的tmp目录下，该目录每次开机都会被清空，因此每次重新启动机器，都需要重新格式化HDFS。

2).DataNode启动失败，要保持name和data的namespaceID、clusterID一致，解决方法：
# cd /usr/lib/hadoop/hadoop-2.6.0/name/current
# vim VERSION
namespaceID=1891453947
clusterID=CID-eb45aa78-5e50-45cb-b6c0-06aa9cd4c476
# cd /usr/lib/hadoop/hadoop-2.6.0/data/current
# vim VERSION
namespaceID=1891453947
clusterID=CID-eb45aa78-5e50-45cb-b6c0-06aa9cd4c476

3). Cannot find configuration directory: /etc/hadoop
WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable]
Error: Cannot find configuration directory: /etc/hadoop
Error: Cannot find configuration directory: /etc/hadoop
解决方法：
# vim /etc/hosts
把127.0.0.1修改为本地IP地址
#127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4
192.168.29.131 localhost localhost.localdomain localhost4 localhost4.localdomain4
::1 localhost localhost.localdomain localhost6 localhost6.localdomain6

192.168.29.131 hadoop1
192.168.29.132 hadoop2
# service network restart

4).WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable
debug调试提示"libc.so.6: version `GLIBC_2.14' not found"
因为系统的gblci版本太低，需要安装2.14以上的版本：
下载glibc2.14，http://www.gnu.org/software/libc/
# tar -zxvf /..目录/glibc-2.14.tar.gz -C /usr/local/glibc
# cd /usr/local/glibc
# ./glibc-2.14/configure --prefix=/usr/local/glibc  //glibc不允许在源码目录树下编译，必须在其他目录configure和build，这样一旦出错将整个目录移除即可
# make
# make install
注意：
安装过程中，如果出现Can't open configuration file /usr/local/glibc/etc/ld.so.conf: No such file or directory
# cp /etc/ld.so.c* /usr/local/glibc/etc

# ln -sf /usr/local/glibc/lib/libc-2.14.so /lib64/libc.so.6 //建立软链指向glibc-2.14
误删libc.so.6解决方法：
# LD_PRELOAD=/lib64/libc-2.12.so ln -s /lib64/libc-2.12.so libc.so.6

# strings /lib64/libc.so.6 | grep GLIBC //显示GLIBC_2.14，说明安装成功
GLIBC_2.2.5
GLIBC_2.2.6
GLIBC_2.3
GLIBC_2.3.2
GLIBC_2.3.3
GLIBC_2.3.4
GLIBC_2.4
GLIBC_2.5
GLIBC_2.6
GLIBC_2.7
GLIBC_2.8
GLIBC_2.9
GLIBC_2.10
GLIBC_2.11
GLIBC_2.12
GLIBC_2.13
GLIBC_2.14
GLIBC_PRIVATE

14.启动YARN
# cd /usr/local/hadoop/hadoop-2.6.0/sbin
# ./start-yarn.sh
# jps //验证YARN启动，此时在hadoop1上运行的进程有：NameNode、SecondaryNameNode、DataNode、NodeManager和ResourceManager
14449 Jps
13970 NodeManager
11878 DataNode
11753 NameNode
14121 ResourceManager
12079 SecondaryNameNode

# jps //此时在hadoop1上运行的进程有：DataNode和NodeManager
25245 Jps
25163 NodeManager
24910 DataNode

15.配置环境变量
# vim /etc/profile
在 pathmunge () { 的上方添加以下内容:
export HADOOP_HOME=/usr/local/hadoop/hadoop-2.6.0
export PATH=.:$HADOOP_HOME/bin:$JAVA_HOME/bin:$PATH
# source /etc/profile

16.测试
# hadoop version
Hadoop 2.6.0
Subversion https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1
Compiled by jenkins on 2014-11-13T21:10Z
Compiled with protoc 2.5.0
From source with checksum 18e43357c8f927c0695f1e9522859d6a
This command was run using /usr/lib/hadoop/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar

17.查看
# cd /usr/local/hadoop/hadoop-2.6.0
# cd lib/native
# file ./libhadoop.so.1.0.0
./libhadoop.so.1.0.0: ELF 64-bit LSB shared object, x86-64, version 1 (SYSV), dynamically linked, not stripped
 ELF 64-bit LSB，说明是64位

1.安装maven
1.1下载maven
下载地址http://maven.apache.org/

1.2解压
# tar -zxvf /..目录/apache-maven-3.3.9-bin.tar.gz -C /usr/local/maven

1.3配置
# vim /etc/profile
配置PATH路径
export MAVEN_HOME=/usr/local/maven/apache-maven-3.3.9
export PATH=$MAVEN_HOME/bin:$PATH
# source /etc/profile
# mvn -version
Apache Maven 3.3.9 (bb52d8502b132ec0a5a3f4c09453c07478323dc5; 2015-11-11T00:41:47+08:00)
Maven home: /usr/local/maven/apache-maven-3.3.9
Java version: 1.8.0_101, vendor: Oracle Corporation
Java home: /usr/lib/java/jdk1.8.0_101/jre
Default locale: en_US, platform encoding: UTF-8
OS name: "linux", version: "2.6.32-504.el6.i686", arch: "i386", family: "unix"

2.安装cmake
2.1下载cmake
下载地址https://cmake.org/

2.2解压
# tar -zxvf /..目录/cmake-3.6.2-Linux-i386.tar.gz -C /usr/local/cmake

2.3配置
# vim /etc/profile
配置PATH路径
export CMAKE_HOME=/usr/local/cmake/cmake-3.6.2-Linux-i386
export PATH=$CMAKE_HOME/bin:$PATH
# source /etc/profile
# cmake -version
cmake version 3.6.2
CMake suite maintained and supported by Kitware (kitware.com/cmake).

3.安装gcc
# yum -y install gcc
# yum -y install glibc-headers
# yum -y install gcc-c++

4.安装protocolbuffer
4.1下载protocolbuffer
下载地址http://developers.google.com/protocol-buffers/

4.2解压
# tar -zxvf /..目录/protobuf-2.5.0.tar.gz -C /usr/local/protobuf

4.3安装
# cd /usr/local/protobuf/protobuf-2.5.0
# ./configure --prefix=/usr/local/protobuf  --指定安装目录
# make
# make install
# vim /etc/profile
添加以下配置：
export PROTOC_HOME=/usr/local/protobuf
export PATH=$PROTOC_HOME/bin:$PATH
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$PROTOC_HOME/lib
# source /etc/profile

4.4验证是否安装成功
# protoc --version
libprotoc 2.5.0

5.编译hadoop
5.1下载hadoop-2.6.0源码
下载地址http://hadoop.apache.org/

5.2.解压
# tar -zxvf /..目录/hadoop-2.6.0-src.tar.gz -C /usr/local/hadoop
# cd /usr/local/hadoop/hadoop-2.6.0-src
# mvn package -Pdist,native -DskipTests -Dtar

3.在hadoop目录下创建子目录
# cd /usr/lib/hadoop/hadoop-2.6.0
# mkdir tmp
# mkdir name
# mkdir data

4.配置hadoop-env.sh
# cd /usr/lib/hadoop/hadoop-2.6.0/etc/hadoop
# vim hadoop-env.sh
设置JAVA_HOME和PATH路径
export JAVA_HOME=/usr/lib/java/jdk1.8.0_101
export PATH=$PATH:/usr/lib/hadoop/hadoop-2.6.0/bin
# source hadoop-env.sh
# hadoop version
如果显示以下内容，就表示配置生效了
Hadoop 2.6.0
Subversion https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1
Compiled by jenkins on 2014-11-13T21:10Z
Compiled with protoc 2.5.0
From source with checksum 18e43357c8f927c0695f1e9522859d6a
This command was run using /usr/lib/hadoop/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar

5.配置yarn-env.sh
# cd /usr/lib/hadoop/hadoop-2.6.0/etc/hadoop
# vim yarn-env.sh
设置JAVA_HOME路径
1).先找到以下内容
# some Java parameters
# export JAVA_HOME=/home/y/libexec/jdk1.6.0/
if [ "$JAVA_HOME" != "" ]; then
  #echo "run java in $JAVA_HOME"
  JAVA_HOME=$JAVA_HOME
fi
2).修改第二行配置
export JAVA_HOME=/usr/lib/java/jdk1.8.0_101
# source yarn-env.sh

6.配置core-site.xml
# cd /usr/lib/hadoop/hadoop-2.6.0/etc/hadoop
# vim core-site.xml
按照如下内容进行配置，fs.default.name的值是hdfs://master:9000
<configuration>
  <property>
    <name>fs.default.name</name>
    <value>hdfs://hadoop1:9000</value>
  </property>

  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://hadoop1:9000</value>
  </property>

  <property>
    <name>io.file.buffer.size</name>
    <value>131072</value>
  </property>

  <property>
    <name>hadoop.tmp.dir</name>
    <value>file:/usr/lib/hadoop/hadoop-2.6.0/tmp</value>
    <description>A base for other temporary directories.</description>
  </property>

  <property>
    <name>hadoop.proxyuser.hduser.hosts</name>
    <value>*</value>
  </property>

  <property>
    <name>hadoop.proxyuser.hduser.groups</name>
    <value>*</value>
  </property>
</configuration>

7.配置hdfs-site.xml
# cd /usr/lib/hadoop/hadoop-2.6.0/etc/hadoop
# vim hdfs-site.xml
按照如下内容进行配置
<configuration>
  <property>
   <name>dfs.namenode.secondary.http-address</name>
   <value>hadoop1:9001</value>
  </property>

  <property>
   <name>dfs.namenode.name.dir</name>
   <value>file:/usr/lib/hadoop/hadoop-2.6.0/name</value>
  </property>

  <property>
   <name>dfs.datanode.data.dir</name>
   <value>file:/usr/lib/hadoop/hadoop-2.6.0/data</value>
  </property>

  <property>
   <name>dfs.replication</name>
   <value>2</value>
  </property>

  <property>
   <name>dfs.webhdfs.enabled</name>
   <value>true</value>
  </property>
</configuration>

8.配置mapred-site.xml
# cd /usr/lib/hadoop/hadoop-2.6.0/etc/hadoop
# cp mapred-site.xml.template mapred-site.xml  --默认情况下不存在mapred-site.xml文件，可以从模板拷贝一份
# vim mapred-site.xml
按照如下内容进行配置
<configuration>
  <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
  </property>

  <property>
    <name>mapreduce.jobhistory.address</name>
    <value>hadoop1:10020</value>
  </property>

  <property>
    <name>mapreduce.jobhistory.webapp.address</name>
    <value>hadoop1:19888</value>
  </property>
</configuration>

9.配置yarn-site.xml
# cd /usr/lib/hadoop/hadoop-2.6.0/etc/hadoop
# vim yarn-site.xml
按照如下内容进行配置
<configuration>
  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
  </property>

  <property>
    <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
    <value>org.apache.hadoop.mapred.ShuffleHandler</value>
  </property>

  <property>
    <name>yarn.resourcemanager.address</name>
    <value>hadoop1:8032</value>
  </property>

  <property>
    <name>yarn.resourcemanager.scheduler.address</name>
    <value>hadoop1:8030</value>
  </property>

  <property>
    <name>yarn.resourcemanager.resource-tracker.address</name>
    <value>hadoop1:8031</value>
  </property>

  <property>
    <name>yarn.resourcemanager.admin.address</name>
    <value>hadoop1:8033</value>
  </property>

  <property>
    <name>yarn.resourcemanager.webapp.address</name>
    <value>hadoop1:8088</value>
  </property>
</configuration>

10.配置Slaves文件
# cd /usr/lib/hadoop/hadoop-2.6.0/etc/hadoop
# vim slaves
在文件中添加以下配置
hadoop1
hadoop2

11.向各节点分发Hadoop程序
# cd /usr/lib/hadoop
# scp -r hadoop-2.6.0 root@hadoop2:/usr/lib/hadoop

12.格式化NameNode
# cd /usr/lib/hadoop/hadoop-2.6.0
# ./bin/hdfs namenode -format
16/10/13 11:12:51 INFO common.Storage: Storage directory /usr/lib/hadoop/hadoop-2.6.0/name has been successfully formatted.
16/10/13 11:12:51 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
16/10/13 11:12:51 INFO util.ExitUtil: Exiting with status 0
16/10/13 11:12:51 INFO namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at localhost/127.0.0.1
************************************************************/

13.启动HDFS
# cd /usr/lib/hadoop/hadoop-2.6.0/sbin
# ./start-dfs.sh
# jps --验证HDFS启动，hadoop1上面运行的进程有：NameNode、SecondaryNameNode和DataNode
13315 Jps
11878 DataNode
11753 NameNode
12079 SecondaryNameNode

注意：
1).NameNode启动失败，解决方法：
HDFS把namenode的格式化信息存在了系统的tmp目录下，该目录每次开机都会被清空，因此每次重新启动机器，都需要重新格式化HDFS。

2).DataNode启动失败，要保持name和data的namespaceID、clusterID一致，解决方法：
# cd /usr/lib/hadoop/hadoop-2.6.0/name/current
# vim VERSION
namespaceID=1891453947
clusterID=CID-eb45aa78-5e50-45cb-b6c0-06aa9cd4c476
# cd /usr/lib/hadoop/hadoop-2.6.0/data/current
# vim VERSION
namespaceID=1891453947
clusterID=CID-eb45aa78-5e50-45cb-b6c0-06aa9cd4c476

14.启动YARN
# cd /usr/lib/hadoop/hadoop-2.6.0/sbin
# ./start-yarn.sh
# jps --验证YARN启动，此时在hadoop1上运行的进程有：NameNode、SecondaryNameNode、DataNode、NodeManager和ResourceManager
14449 Jps
13970 NodeManager
11878 DataNode
11753 NameNode
14121 ResourceManager
12079 SecondaryNameNode

15.配置环境变量
# vim /etc/profile
在 pathmunge () { 的上方添加以下内容:
export HADOOP_HOME=/usr/lib/hadoop/hadoop-2.6.0
export PATH=.:$HADOOP_HOME/bin:$JAVA_HOME/bin:$PATH
# source /etc/profile

16.测试
# hadoop version
Hadoop 2.6.0
Subversion https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1
Compiled by jenkins on 2014-11-13T21:10Z
Compiled with protoc 2.5.0
From source with checksum 18e43357c8f927c0695f1e9522859d6a
This command was run using /usr/lib/hadoop/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar

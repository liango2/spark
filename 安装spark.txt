1.下载spark
下载地址http://spark.apache.org/

2.解压
# tar -zxvf /..目录/spark-2.0.1-bin-hadoop2.6.tgz -C /usr/local/spark

3.配置/etc/profile
# vim /etc/profile
export SPARK_HOME=/usr/local/spark/spark-2.0.1-bin-hadoop2.6
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
# source /etc/profile

4.配置conf/slaves
# cd /usr/local/spark/spark-2.0.1-bin-hadoop2.6/conf
# vim slaves
加入slave配置节点
hadoop1
hadoop2

5.配置conf/spark-env.sh
# cd /usr/local/spark/spark-2.0.1-bin-hadoop2.6/conf
# cp spark-env.sh.template spark-env.sh
# vim spark-env.sh
设置hadoop1为Master节点
export JAVA_HOME=/usr/local/java/jdk1.7.0_80    #得明确指定java_home
export HADOOP_HOME=/usr/local/hadoop/hadoop-2.6.0   #得明确指定HADOOP_HOME，否则会有错误：HADOOP_HOME or hadoop.home.dir are not set.
export SPARK_MASTER_IP=hadoop1
export SPARK_MASTER_PORT=7077
export SPARK_WORKER_CORES=1
export SPARK_WORKER_INSTANCES=1
export SPARK_WORKER_MEMORY=512M
# source spark-env.sh

6.向各节点分发spark程序
# cd /usr/local/spark
# scp -r spark-2.0.1-bin-hadoop2.6 root@hadoop2:/usr/local/spark/spark-2.0.1-bin-hadoop2.6

7.启动Spark
# cd /usr/local/spark/spark-2.0.1-bin-hadoop2.6/sbin
# ./start-all.sh
starting org.apache.spark.deploy.master.Master, logging to /usr/lib/spark/spark-2.0.1-bin-hadoop2.6/logs/spark-root-org.apache.spark.deploy.master.Master-1-hadoop1.out
hadoop2: starting org.apache.spark.deploy.worker.Worker, logging to /usr/lib/spark/spark-2.0.1-bin-hadoop2.6/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-hadoop2.out
hadoop1: starting org.apache.spark.deploy.worker.Worker, logging to /usr/lib/spark/spark-2.0.1-bin-hadoop2.6/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-hadoop1.out

8. 验证
此时在hadoop1上面运行的进程有：Worker和Master
# jps
2921 Master
2988 Worker
3087 Jps

在hadoop2上面运行的进程有只有Worker
# jps
5537 Worker
5620 Jps

查看hadoop1节点网络情况，8080和7077端口处于侦听状态
# netstat -nlt
tcp        0      0 ::ffff:127.0.0.1:7077       :::*                        LISTEN      
tcp        0      0 :::111                      :::*                        LISTEN      
tcp        0      0 :::8080                     :::*                        LISTEN      

在浏览器中输入 http://hadoop1:8080，可以进入Spark集群状态页面
    URL: spark://hadoop1:7077
    REST URL: spark://hadoop1:6066 (cluster mode)
    Alive Workers: 2
    Cores in use: 2 Total, 0 Used
    Memory in use: 1024.0 MB Total, 0.0 B Used
    Applications: 0 Running, 0 Completed
    Drivers: 0 Running, 0 Completed
    Status: ALIVE
显示以下内容就说明spark集群启动成功：
1).Alive Workers: 2  
2).Workers列表包含hadoop1、hadoop2的地址

9.验证客户端连接
# cd /usr/local/spark/spark-2.0.1-bin-hadoop2.6/bin
# ./spark-shell --master spark://hadoop1:7077 --executor-memory 500m
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.0.1
      /_/
         
Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_80)
Type in expressions to have them evaluated.
Type :help for more information.

scala> 

注意：
1.Spark的日志配置
# cd /usr/local/spark/spark-2.0.1-bin-hadoop2.6/conf
# vim log4j.properties
log4j.rootCategory=INFO, console    //显示所有信息
log4j.rootCategory=WARN, console    //显示警告信息
log4j.rootCategory=DEBUG, console    //显示调试信息
